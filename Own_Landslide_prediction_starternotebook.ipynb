{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Landslide_prediction_starternotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center> Welcome to the Landslide Prediction Challenge</h2></center>\n",
        "\n",
        "A landslide is the movement of a mass of rock, debris, or earth(soil) down a slope. As a common natural hazard, it can lead to significant losses of human lives and properties.\n",
        "\n",
        "\n",
        "Hong Kong, one of the hilly and densely populated cities in the world, is frequently affected by extreme rainstorms, making it highly susceptible to rain-induced natural terrain landslides\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?export=view&id=1-8sSI75AG3HM89nDJEwo6_KJbAEUXS-r\">\n",
        "\n",
        "The common practice of identifying landslides is visual interpretation which, however, is labor-intensive and time-consuming.\n",
        "\n",
        "***Thus, this hack will focus on automating the landslide identification process using artificial intelligence techniques***\n",
        "\n",
        "This will be achieved by using high-resolution terrain information to perform the terrain-based landslide identification. Other auxiliary data such as the lithology of the surface materials and rainfall intensification factor are also provided.\n"
      ],
      "metadata": {
        "id": "DqgLGfMXMQXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table of contents:\n",
        "\n",
        "1. [Import relevant libraries](#Libraries)\n",
        "2. [Load files](#Load)\n",
        "3. [Preview files](#Preview)\n",
        "4. [Data dictionary](#Dictionary)\n",
        "5. [Data exploration](#Exploration)\n",
        "6. [Target distribution](#Target)\n",
        "7. [Outliers](#Outliers)\n",
        "8. [Correlations](#Correlations)\n",
        "9. [Model training](#Model)\n",
        "10. [Test set predictions](#Predictions)\n",
        "11. [Creating a submission file](#Submission)\n",
        "12. [Tips to improve model performance](#Tips)"
      ],
      "metadata": {
        "id": "dtRUPR6qPKMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Libraries\"></a>\n",
        "## 1. Import relevant libraries"
      ],
      "metadata": {
        "id": "Sl-lBcUwJZX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score, classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "pd.set_option('display.max_columns', None)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "AGkN_YDYrXeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Load\"></a>\n",
        "## 2. Load files"
      ],
      "metadata": {
        "id": "31jrsc4hJdw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/leopoldpoldus/UNEP-STARTHACK22.git"
      ],
      "metadata": {
        "id": "8x22iuHT_P3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files to pandas dataframes\n",
        "train = pd.read_csv('/content/UNEP-STARTHACK22/Dataset/Train.csv')\n",
        "test = pd.read_csv('/content/UNEP-STARTHACK22/Dataset/Test.csv')\n",
        "sample_submission = pd.read_csv('/content/UNEP-STARTHACK22/Dataset/Sample submission.csv')"
      ],
      "metadata": {
        "id": "pK406xHWrk3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Preview\"></a>\n",
        "## 3. Preview files"
      ],
      "metadata": {
        "id": "yv67GA1KJgiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first five rows of the train set\n",
        "train.head()"
      ],
      "metadata": {
        "id": "4w9rZSXdrk0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first five rows of the test set\n",
        "test.head()"
      ],
      "metadata": {
        "id": "25pTbez9rkxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how the submission file should look like\n",
        "sample_submission.head()"
      ],
      "metadata": {
        "id": "Q8IMnGZZiFFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Dictionary\"></a>\n",
        "## 4. Data Dictionary\n",
        "<figure>\n",
        "<img src = \"https://drive.google.com/uc?export=view&id=1T_XBSH6ozmhGiDz_nL4bQvvonHUpbCfW\" height = \"200\">\n",
        "<img src = \"https://drive.google.com/uc?export=view&id=13nSrrIowiFPjAgiR--Nd4cHLVwvXFaFj\" height = \"400\">"
      ],
      "metadata": {
        "id": "_1pwPYU3Hs20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape and size of train and test set\n",
        "train.shape, test.shape, sample_submission.shape"
      ],
      "metadata": {
        "id": "lRxHcvQYrkvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Exploration\"></a>\n",
        "## 5. Data exploration"
      ],
      "metadata": {
        "id": "gvAD-Ch3JSdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check statistical summaries of the train set\n",
        "train.describe()"
      ],
      "metadata": {
        "id": "or6pAQugrksw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - There is a very high correlation between features extracted from the same location"
      ],
      "metadata": {
        "id": "PNAalb-_1LUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elevation correlations\n",
        "plt.figure(figsize = (20, 12))\n",
        "sample_elevations = ['1_elevation',\t'2_elevation',\t'3_elevation',\t'4_elevation',\t'5_elevation']\n",
        "sns.pairplot(train[sample_elevations], kind=\"scatter\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xq-DHCxN1V4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check statistical summaries of the test set\n",
        "test.describe()"
      ],
      "metadata": {
        "id": "K8omCIZ7rkqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any missing values\n",
        "train.isnull().sum().any(), test.isnull().sum().any()"
      ],
      "metadata": {
        "id": "UIr3bZ2esc8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "train.duplicated().any(), test.duplicated().any()"
      ],
      "metadata": {
        "id": "BUVBIDlEsc5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Target\"></a>\n",
        "## 6. Target variable distribution"
      ],
      "metadata": {
        "id": "Aq3Ayzs6JKCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check distribution of the target variabe\n",
        "train.Label.value_counts(normalize = True)"
      ],
      "metadata": {
        "id": "ud4fMH9isc24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.set_style('darkgrid')\n",
        "# plt.figure(figsize=(7, 6))\n",
        "# sns.countplot(x= train.Label)\n",
        "# plt.title('Target Variable Distribution')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Q-kydrKeubcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is highly imbalanced with the majority class having 75% and the minority class 25%\n",
        "\n",
        "Some techiques in handling class imbalance include;\n",
        " 1. Using SMOTE to create synthetic data to reduce imbalanceness\n",
        " 2. Undersampling the majority class\n",
        " 3. Oversampling the minority class\n",
        " 4. Giving more weight to minority class during modelling"
      ],
      "metadata": {
        "id": "q_ZYM-6MrA64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Outliers\"></a>\n",
        "## 7. Outliers"
      ],
      "metadata": {
        "id": "HdANOHp0zg5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring some features for cell 1\n",
        "explore_cols =  ['1_elevation', '1_aspect', '1_slope', '1_placurv', '1_procurv', '1_lsfactor']\n",
        "explore_cols"
      ],
      "metadata": {
        "id": "rvgAIG7Xkinh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting boxplots for each of the numerical columns\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 3, figsize = (20, 10))\n",
        "fig.suptitle('Box plots showing outliers', y= 0.93, fontsize = 15)\n",
        "\n",
        "for ax, data, name in zip(axes.flatten(), train, explore_cols):\n",
        "  sns.boxplot(train[name], ax = ax)\n",
        "\n"
      ],
      "metadata": {
        "id": "m-KQm8cLjEV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Elevation, IsFactor, Placurv, curve and slope have some outliers.\n",
        " The aspect feature has no outliers.\n",
        " \n",
        " Some of the techniques you can use to handle outliers include:\n",
        "  1. Log transformations, scaling, box-cox transformations...\n",
        "  2. Dropping the outliers\n",
        "  3. Replacing the outliers with mean, median, mode or any other aggregates"
      ],
      "metadata": {
        "id": "MPZ3gzR7o6q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Correlations\"></a>\n",
        "## 8. Correlations"
      ],
      "metadata": {
        "id": "Gw9ZEYplznFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Type of correlations \n",
        "# plt.figure(figsize = (20, 12))\n",
        "# sns.pairplot(train[explore_cols], kind=\"scatter\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "65rt63dqlFsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is no correlation for most of the features, how can you capture this information for modelling...\n",
        "- Which information can you derive from this correlations"
      ],
      "metadata": {
        "id": "jW5D5TzRohgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantify correlations\n",
        "# corr = train[explore_cols].corr()\n",
        "# plt.figure(figsize = (13, 8))\n",
        "# sns.heatmap(corr, cmap='RdYlGn', annot = True, center = 0)\n",
        "# plt.title('Correlogram', fontsize = 15, color = 'darkgreen')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "EYxXJhb_lFqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - There is a strong positive correlation of approximately 0.8 between slope and IsFactor\n",
        " - There is some negative correlation between IsFactor and placurv\n",
        " - The IsFactor variable is correlated most of the other features, why is this?"
      ],
      "metadata": {
        "id": "G6UtDqJAn4X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "YWwpZOqdecDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encode Geo Categorie -> useless"
      ],
      "metadata": {
        "id": "jIEFdzFP8lJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train[train.Label == 0]['1_slope'], label = 'no_slide')\n",
        "plt.hist(train[train.Label == 1]['1_slope'], label = 'land_slide')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "XaewRtUCAbM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train[train.Label == 0]['1_slope']/train[train.Label == 0]['1_twi'], label = 'no_slide')\n",
        "plt.hist(train[train.Label == 1]['1_slope']/train[train.Label == 1]['1_twi'], label = 'land_slide')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "7Xty6GjURkIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train[train.Label == 0]['1_procurv'], label = 'no_slide')\n",
        "plt.hist(train[train.Label == 1]['1_procurv'], label = 'land_slide')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "RIs0V4tbRMGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train[train.Label == 0][[f'{i}_slope' for i in range(1, 26)]].mean(axis = 1), label = 'no_slide')\n",
        "plt.hist(train[train.Label == 1][[f'{i}_slope' for i in range(1, 26)]].mean(axis = 1), label = 'land_slide')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "sOmTzhsgBd7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(train[train.Label == 0]['1_slope'],\n",
        "            train[train.Label == 0]['1_geology'])\n",
        "            "
      ],
      "metadata": {
        "id": "EQmuU9MKCopY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# enc = OneHotEncoder(handle_unknown='ignore')\n",
        "# geology_features = [f'{i}_geology' for i in range(1, 26)]\n",
        "# enc.fit(train[geology_features])\n",
        "# new_cols = []\n",
        "# for tile in range(1, 26):\n",
        "#   for cat in range (1, 8):\n",
        "#     new_cols.append(f'{tile}_geo_cat_{cat}')\n",
        "# # print(new_cols)\n",
        "\n",
        "\n",
        "# one_hot_geology_train = pd.DataFrame(enc.transform(train[geology_features]).toarray(), columns = new_cols)\n",
        "# one_hot_geology_test = pd.DataFrame(enc.transform(test[geology_features]).toarray(), columns = new_cols)\n",
        "\n",
        "# new_cols_rearanged = []\n",
        "# for cat in range (1, 8):\n",
        "#   for tile in range(1, 26):\n",
        "#     new_cols_rearanged.append(f'{tile}_geo_cat_{cat}')\n",
        "\n",
        "# one_hot_geology_train = one_hot_geology_train[new_cols_rearanged] \n",
        "# one_hot_geology_train['Sample_ID'] = train['Sample_ID']\n",
        "# one_hot_geology_test = one_hot_geology_test[new_cols_rearanged]\n",
        "# one_hot_geology_test['Sample_ID'] = test['Sample_ID']\n",
        "\n",
        "# test_new = test.merge(one_hot_geology_test)\n",
        "# test_new = test_new[list(test.columns)+list(new_cols_rearanged)]\n",
        "# train_new = train.merge(one_hot_geology_train)\n",
        "# train_new = train_new[list(train.columns[:-1])+list(new_cols_rearanged)+['Label']]\n"
      ],
      "metadata": {
        "id": "BIFOYkPjdy2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_df_to_train_and_test(df_train, df_test, train, test, name):\n",
        "  df_test.columns = [f'{i}_{name}' for i in range(1, 26)]\n",
        "  df_train.columns = [f'{i}_{name}' for i in range(1, 26)]\n",
        "\n",
        "  df_train['Sample_ID'] = train['Sample_ID']\n",
        "  df_test['Sample_ID'] = test['Sample_ID']\n",
        "\n",
        "  test_new = test.merge(df_test)\n",
        " \n",
        "  test_new = test_new[list(test.columns)+list(df_test.columns)[:-1]] # TODO macht das was?\n",
        "  # print(test_new.columns)\n",
        "  train_new = train.merge(df_train)\n",
        "  train_new = train_new[list(train.columns[:-1])+list(df_train.columns[:-1])+['Label']]\n",
        "  # print(train_new.columns)\n",
        "  return train_new, test_new\n"
      ],
      "metadata": {
        "id": "dLQYQasCAvh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_combined_metrics_for_tiles(cat, train, test):\n",
        "  df_train = train[[f'{i}_{cat}' for i in range(1, 26)]]\n",
        "  df_test = test[[f'{i}_{cat}' for i in range(1, 26)]]\n",
        "  mean_train = df_train.mean(axis = 1)\n",
        "  mean_test = df_test.mean(axis = 1)\n",
        "  \n",
        "  return mean_train, mean_test\n"
      ],
      "metadata": {
        "id": "Ij9oV9TduDO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean_train, mean_test = get_combined_metrics_for_tiles('elevation', train_new, test_new)\n",
        "# "
      ],
      "metadata": {
        "id": "4qn0qEmevEnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train_averaged = df"
      ],
      "metadata": {
        "id": "6hqZdivKvqzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode elevation as categorical variable"
      ],
      "metadata": {
        "id": "PxayOa9q8sAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change Elevation to categorical\n",
        "\n",
        "df_elevation_train = train[[f'{i}_elevation' for i in range(1, 26)]]\n",
        "\n",
        "df_elevation_train[df_elevation_train < 101] = 1\n",
        "df_elevation_train[list(101 >= df_elevation_train) and list(df_elevation_train < 192)] = 2\n",
        "df_elevation_train[list(192 >= df_elevation_train) and list(df_elevation_train < 312)] = 3\n",
        "df_elevation_train[list(312 >= df_elevation_train)] = 4\n",
        "\n",
        "\n",
        "df_elevation_test = test[[f'{i}_elevation' for i in range(1, 26)]]\n",
        "\n",
        "df_elevation_test[df_elevation_test < 101] = 1\n",
        "df_elevation_test[list(101 >= df_elevation_test) and list(df_elevation_test < 192)] = 2\n",
        "df_elevation_test[list(192 >= df_elevation_test) and list(df_elevation_test < 312)] = 3\n",
        "df_elevation_test[list(312 >= df_elevation_test)] = 4\n",
        "\n",
        "# for i in range(1,26):\n",
        "#   df_elevation[i] = np.where(df_elevation[i].between(101,192), 2, df_elevation[i])\n",
        "#   df_elevation[i] = np.where(df_elevation[i].between(193,312), 3, df_elevation[i])\n",
        "#   df_elevation[df_elevation >= 312] = 4"
      ],
      "metadata": {
        "id": "B8JjZudSi5SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_new, test_new = add_df_to_train_and_test(df_elevation_train, df_elevation_test, train, test, 'catgoricalelevation')"
      ],
      "metadata": {
        "id": "EW_PP-TylmUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def multiply_two_cat(cat_1, cat_2, orig_df):\n",
        "  df_1 = orig_df[[f'{i}_{cat_1}' for i in range(1, 26)]]\n",
        "  df_2 = orig_df[[f'{i}_{cat_2}' for i in range(1, 26)]]\n",
        "  # df_2[df_2 < 40] = 0\n",
        "  df_2.columns = list(range(1, 26))\n",
        "  df_1.columns = list(range(1, 26))\n",
        "  return df_1.mul(df_2)\n",
        "\n",
        "def apply_lambda_to_cat(f, cat, orig_df):\n",
        "  df = orig_df[[f'{i}_{cat}' for i in range(1, 26)]]\n",
        "  return df.apply(f)\n",
        "\n",
        "def apply_lambda_to_train_and_test(f, cat, train, test, name):\n",
        "  train_ = apply_lambda_to_cat(f, cat, train)\n",
        "  test_ = apply_lambda_to_cat(f, cat, test)\n",
        "  return add_df_to_train_and_test(train_, test_, train, test, name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uCHJ49tJDj7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add tan and tanh transformations to slope"
      ],
      "metadata": {
        "id": "BvNKRmr489jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geo_dict = {\n",
        "    1: 0.1,\n",
        "    2: 0.2,\n",
        "    3: 1,\n",
        "    4: 0,\n",
        "    5: 0.3,\n",
        "    6: 0,\n",
        "    7: 0.1,\n",
        "}\n",
        "\n",
        "# def clean_outliers(array, min, max):\n",
        "#   mn = np.mean(array)\n",
        "#   for x in array:\n",
        "#     if x > max or x < min:\n",
        "#       x = mn\n",
        "#   return np.array(array)\n",
        "\n",
        "\n",
        "train_new, test_new = apply_lambda_to_train_and_test(lambda x: [geo_dict[i] for i in x], 'geology', train_new, test_new, 'geo_rescored')\n",
        "\n",
        "train_new, test_new = apply_lambda_to_train_and_test(lambda x: [np.mean(x) if (i > 312.0 or i < 101.75) else i for i in x], 'elevation', train_new, test_new, 'elevationnoout')\n",
        "\n",
        "train_new, test_new = apply_lambda_to_train_and_test(lambda x: [45.0 if (i > 50.0) else i for i in x], 'slope', train_new, test_new, 'slopenoout')\n",
        "\n",
        "train_new, test_new = apply_lambda_to_train_and_test(lambda x: np.tan(2*np.pi/360 * x), 'slope', train_new, test_new, 'slope_tan')\n",
        "\n",
        "train_new, test_new = apply_lambda_to_train_and_test(lambda x: np.tanh(2*np.pi/360 * x), 'slope', train_new, test_new, 'slope_tanh')\n",
        "\n",
        "train_new, test_new = apply_lambda_to_train_and_test(lambda x: np.log(x), 'slope', train_new, test_new, 'slope_log')\n"
      ],
      "metadata": {
        "id": "WaQItvPr5f3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# slope_tan_train"
      ],
      "metadata": {
        "id": "X-OVPtDZ7q27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ = multiply_two_cat('geology', 'slope', train)\n",
        "# test_ = multiply_two_cat('geology', 'slope', test)\n",
        "# train_new, test_new = add_df_to_train_and_test(train_, test_, train, test, 'geovsslope')\n",
        "\n",
        "# train_ = multiply_two_cat('twi', 'slope', train)\n",
        "# test_ = multiply_two_cat('twi', 'slope', test)\n",
        "# train_new, test_new = add_df_to_train_and_test(train_, test_, train, test, '-twi-vs-slope')\n",
        "\n",
        "# train_new, test_new = train_new, test_new\n"
      ],
      "metadata": {
        "id": "YXJ2phiAESDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "scaler = StandardScaler()\n",
        "# scaler = RobustScaler()\n",
        "\n",
        "new_df = train_new[train_new.columns[1:-1]].copy(deep=True)\n",
        "new_df = pd.DataFrame(scaler.fit_transform(new_df))\n",
        "new_df.columns = train_new.columns[1:-1]\n",
        "new_df['Sample_ID'] = train_new['Sample_ID']\n",
        "new_df = new_df[[new_df.columns[-1]]+list(new_df.columns[:-1])]\n",
        "new_df['Label'] = train_new['Label']\n",
        "train_new = new_df \n",
        "test_new[train_new.columns[1:-1]] = pd.DataFrame(scaler.transform(test_new[train_new.columns[1:-1]]))"
      ],
      "metadata": {
        "id": "IF_hYTa_I8s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def block_col(train, test, col):\n",
        "  new_train = train[filter(lambda x: col not in x, train.columns)]\n",
        "  new_test = test[filter(lambda x: col not in x, test.columns)]\n",
        "  return new_train, new_test"
      ],
      "metadata": {
        "id": "_ee7s2Uucz4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_new, test_new = block_col(train_new, test_new, '_')\n",
        "# train_new, test_new = block_col(train_new, test_new, '_elevation')\n",
        "# train_new, test_new = block_col(train_new, test_new, '_aspect')"
      ],
      "metadata": {
        "id": "SnEGwFgHecwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Model\"></a>\n",
        "## 9. Feature Selection"
      ],
      "metadata": {
        "id": "GbN3sDZxJHVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Select main columns to be used in training\n",
        "# main_cols = train_new.columns.difference(['Sample_ID', 'Label'])\n",
        "# X = train_new[main_cols]\n",
        "# y = train_new.Label\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=2022)\n",
        "\n",
        "# number_cat = int((len(test_new.columns)-1)/25)\n",
        "# existing_cols = []\n",
        "# best_feats = []\n",
        "# best_scores = []\n",
        "# categories_as_num = list(range(number_cat))\n",
        "# for feat in range(number_cat):\n",
        "#   best_feat = 0\n",
        "#   best_score = 0\n",
        "  \n",
        "#   best_cols = []\n",
        "#   for i in categories_as_num:\n",
        "#     # if i == 9:\n",
        "#     #   new_cols_i = list(train_new.columns[9*25+1:-1])\n",
        "#     #   cols_i = new_cols_i + existing_cols\n",
        "#     # else:\n",
        "#     new_cols_i = list(train_new.columns[i*25+1:(i+1)*25+1])\n",
        "#     # print(new_cols_i)\n",
        "\n",
        "#     cols_i = new_cols_i + existing_cols\n",
        "#     X_i = train_new[cols_i]\n",
        "#     y_i = train_new.Label\n",
        "    \n",
        "\n",
        "#     # Split data into train and test sets\n",
        "#     X_i_train, X_i_test, y_i_train, y_i_test = train_test_split(X_i,y_i,test_size=0.3, random_state=2022)\n",
        "\n",
        "#     # Train model\n",
        "#     model = RandomForestClassifier(random_state = 2022)\n",
        "#     model.fit(X_i_train, y_i_train)\n",
        "\n",
        "#     # Make predictions\n",
        "#     y_i_pred = model.predict(X_i_test)\n",
        "\n",
        "#     # Check the auc score of the model\n",
        "#     # print(f'RandomForest F1 score on the X_test is: {f1_score(y_i_test, y_i_pred)}\\n')\n",
        "#     if f1_score(y_i_test, y_i_pred) > best_score:\n",
        "#       name = new_cols_i[0].split('_')[1] \n",
        "#       best_score = f1_score(y_i_test, y_i_pred)\n",
        "#       best_feat = i\n",
        "#       best_cols = new_cols_i\n",
        "\n",
        "#     # print classification report\n",
        "#     # print(classification_report(y_i_test, y_i_pred))\n",
        "#   existing_cols = existing_cols + best_cols\n",
        "#   best_feats.append(best_feat)\n",
        "#   best_scores.append(best_score)\n",
        "#   categories_as_num.remove(best_feat)\n",
        "#   print(f'{best_feat} : {name}: {best_score}')\n"
      ],
      "metadata": {
        "id": "GewJbaCNscz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(best_scores)"
      ],
      "metadata": {
        "id": "CFg58Za6TsLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# existing_cols = ['1_slope', '2_slope', '3_slope', '4_slope', '5_slope', '6_slope', '7_slope', '8_slope', '9_slope', '10_slope', '11_slope', '12_slope', '13_slope', '14_slope', '15_slope', '16_slope', '17_slope', '18_slope', '19_slope', '20_slope', '21_slope', '22_slope', '23_slope', '24_slope', '25_slope', '1_geology', '2_geology', '3_geology', '4_geology', '5_geology', '6_geology', '7_geology', '8_geology', '9_geology', '10_geology', '11_geology', '12_geology', '13_geology', '14_geology', '15_geology', '16_geology', '17_geology', '18_geology', '19_geology', '20_geology', '21_geology', '22_geology', '23_geology', '24_geology', '25_geology', '1_elevation', '2_elevation', '3_elevation', '4_elevation', '5_elevation', '6_elevation', '7_elevation', '8_elevation', '9_elevation', '10_elevation', '11_elevation', '12_elevation', '13_elevation', '14_elevation', '15_elevation', '16_elevation', '17_elevation', '18_elevation', '19_elevation', '20_elevation', '21_elevation', '22_elevation', '23_elevation', '24_elevation', '25_elevation', '1_sdoif', '2_sdoif', '3_sdoif', '4_sdoif', '5_sdoif', '6_sdoif', '7_sdoif', '8_sdoif', '9_sdoif', '10_sdoif', '11_sdoif', '12_sdoif', '13_sdoif', '14_sdoif', '15_sdoif', '16_sdoif', '17_sdoif', '18_sdoif', '19_sdoif', '20_sdoif', '21_sdoif', '22_sdoif', '23_sdoif', '24_sdoif', '25_sdoif', '1_lsfactor', '2_lsfactor', '3_lsfactor', '4_lsfactor', '5_lsfactor', '6_lsfactor', '7_lsfactor', '8_lsfactor', '9_lsfactor', '10_lsfactor', '11_lsfactor', '12_lsfactor', '13_lsfactor', '14_lsfactor', '15_lsfactor', '16_lsfactor', '17_lsfactor', '18_lsfactor', '19_lsfactor', '20_lsfactor', '21_lsfactor', '22_lsfactor', '23_lsfactor', '24_lsfactor', '25_lsfactor', '1_placurv', '2_placurv', '3_placurv', '4_placurv', '5_placurv', '6_placurv', '7_placurv', '8_placurv', '9_placurv', '10_placurv', '11_placurv', '12_placurv', '13_placurv', '14_placurv', '15_placurv', '16_placurv', '17_placurv', '18_placurv', '19_placurv', '20_placurv', '21_placurv', '22_placurv', '23_placurv', '24_placurv', '25_placurv', '1_twi', '2_twi', '3_twi', '4_twi', '5_twi', '6_twi', '7_twi', '8_twi', '9_twi', '10_twi', '11_twi', '12_twi', '13_twi', '14_twi', '15_twi', '16_twi', '17_twi', '18_twi', '19_twi', '20_twi', '21_twi', '22_twi', '23_twi', '24_twi', '25_twi', '1_procurv', '2_procurv', '3_procurv', '4_procurv', '5_procurv', '6_procurv', '7_procurv', '8_procurv', '9_procurv', '10_procurv', '11_procurv', '12_procurv', '13_procurv', '14_procurv', '15_procurv', '16_procurv', '17_procurv', '18_procurv', '19_procurv', '20_procurv', '21_procurv', '22_procurv', '23_procurv', '24_procurv', '25_procurv', '1_aspect', '2_aspect', '3_aspect', '4_aspect', '5_aspect', '6_aspect', '7_aspect', '8_aspect', '9_aspect', '10_aspect', '11_aspect', '12_aspect', '13_aspect', '14_aspect', '15_aspect', '16_aspect', '17_aspect', '18_aspect', '19_aspect', '20_aspect', '21_aspect', '22_aspect', '23_aspect', '24_aspect', '25_aspect', '1_geo_cat_1', '2_geo_cat_1', '3_geo_cat_1', '4_geo_cat_1', '5_geo_cat_1', '6_geo_cat_1', '7_geo_cat_1', '8_geo_cat_1', '9_geo_cat_1', '10_geo_cat_1', '11_geo_cat_1', '12_geo_cat_1', '13_geo_cat_1', '14_geo_cat_1', '15_geo_cat_1', '16_geo_cat_1', '17_geo_cat_1', '18_geo_cat_1', '19_geo_cat_1', '20_geo_cat_1', '21_geo_cat_1', '22_geo_cat_1', '23_geo_cat_1', '24_geo_cat_1', '25_geo_cat_1', '1_geo_cat_2', '2_geo_cat_2', '3_geo_cat_2', '4_geo_cat_2', '5_geo_cat_2', '6_geo_cat_2', '7_geo_cat_2', '8_geo_cat_2', '9_geo_cat_2', '10_geo_cat_2', '11_geo_cat_2', '12_geo_cat_2', '13_geo_cat_2', '14_geo_cat_2', '15_geo_cat_2', '16_geo_cat_2', '17_geo_cat_2', '18_geo_cat_2', '19_geo_cat_2', '20_geo_cat_2', '21_geo_cat_2', '22_geo_cat_2', '23_geo_cat_2', '24_geo_cat_2', '25_geo_cat_2', '1_geo_cat_3', '2_geo_cat_3', '3_geo_cat_3', '4_geo_cat_3', '5_geo_cat_3', '6_geo_cat_3', '7_geo_cat_3', '8_geo_cat_3', '9_geo_cat_3', '10_geo_cat_3', '11_geo_cat_3', '12_geo_cat_3', '13_geo_cat_3', '14_geo_cat_3', '15_geo_cat_3', '16_geo_cat_3', '17_geo_cat_3', '18_geo_cat_3', '19_geo_cat_3', '20_geo_cat_3', '21_geo_cat_3', '22_geo_cat_3', '23_geo_cat_3', '24_geo_cat_3', '25_geo_cat_3', '1_geo_cat_4', '2_geo_cat_4', '3_geo_cat_4', '4_geo_cat_4', '5_geo_cat_4', '6_geo_cat_4', '7_geo_cat_4', '8_geo_cat_4', '9_geo_cat_4', '10_geo_cat_4', '11_geo_cat_4', '12_geo_cat_4', '13_geo_cat_4', '14_geo_cat_4', '15_geo_cat_4', '16_geo_cat_4', '17_geo_cat_4', '18_geo_cat_4', '19_geo_cat_4', '20_geo_cat_4', '21_geo_cat_4', '22_geo_cat_4', '23_geo_cat_4', '24_geo_cat_4', '25_geo_cat_4', '1_geo_cat_5', '2_geo_cat_5', '3_geo_cat_5', '4_geo_cat_5', '5_geo_cat_5', '6_geo_cat_5', '7_geo_cat_5', '8_geo_cat_5', '9_geo_cat_5', '10_geo_cat_5', '11_geo_cat_5', '12_geo_cat_5', '13_geo_cat_5', '14_geo_cat_5', '15_geo_cat_5', '16_geo_cat_5', '17_geo_cat_5', '18_geo_cat_5', '19_geo_cat_5', '20_geo_cat_5', '21_geo_cat_5', '22_geo_cat_5', '23_geo_cat_5', '24_geo_cat_5', '25_geo_cat_5', '1_geo_cat_6', '2_geo_cat_6', '3_geo_cat_6', '4_geo_cat_6', '5_geo_cat_6', '6_geo_cat_6', '7_geo_cat_6', '8_geo_cat_6', '9_geo_cat_6', '10_geo_cat_6', '11_geo_cat_6', '12_geo_cat_6', '13_geo_cat_6', '14_geo_cat_6', '15_geo_cat_6', '16_geo_cat_6', '17_geo_cat_6', '18_geo_cat_6', '19_geo_cat_6', '20_geo_cat_6', '21_geo_cat_6', '22_geo_cat_6', '23_geo_cat_6', '24_geo_cat_6', '25_geo_cat_6', '1_geo_cat_7', '2_geo_cat_7', '3_geo_cat_7', '4_geo_cat_7', '5_geo_cat_7', '6_geo_cat_7', '7_geo_cat_7', '8_geo_cat_7', '9_geo_cat_7', '10_geo_cat_7', '11_geo_cat_7', '12_geo_cat_7', '13_geo_cat_7', '14_geo_cat_7', '15_geo_cat_7', '16_geo_cat_7', '17_geo_cat_7', '18_geo_cat_7', '19_geo_cat_7', '20_geo_cat_7', '21_geo_cat_7', '22_geo_cat_7', '23_geo_cat_7', '24_geo_cat_7', '25_geo_cat_7']\n",
        "def create_list_with_cats(cat_list):\n",
        "  return_list = []\n",
        "  for element in cat_list:\n",
        "    return_list += [f'{i}_{element}' for i in range(1, 26)]\n",
        "  return return_list\n",
        "cats = ['slopenoout', 'geology', 'elevation', 'sdoif', 'twi', 'aspect', 'placurv']\n",
        "existing_cols = create_list_with_cats(cats)"
      ],
      "metadata": {
        "id": "fp3OC0O8zuLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "n = len(cats)\n",
        "\n",
        "train_new = train_new[['Sample_ID']+existing_cols+['Label']]\n",
        "\n",
        "test_new = test_new[['Sample_ID']+existing_cols]\n",
        "\n",
        "main_cols = train_new.columns.difference(['Sample_ID', 'Label'])\n",
        "\n",
        "X = train_new[main_cols]\n",
        "y = train_new.Label\n",
        "\n",
        "\n",
        "\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# ros = RandomUnderSampler(random_state=0)\n",
        "# ros.fit(X, y)\n",
        "# X, y = ros.fit_resample(X, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "4uC7DlCS-min"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "# kf = KFold(n_splits=2, random_state = 2022, shuffle=True)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=2022)"
      ],
      "metadata": {
        "id": "5R-MEC23XxXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(train_new['Label'])\n",
        "pos = sum(train_new['Label'])\n",
        "neg = total - pos\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "# weight_for_0 = 1\n",
        "# weight_for_1 = 1\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n"
      ],
      "metadata": {
        "id": "dKqUKlY0CebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1(precision, recall):\n",
        "  return 2 * (precision * recall)/ (precision + recall)\n"
      ],
      "metadata": {
        "id": "0DutF_fm19jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "shape = n*25\n",
        "\n",
        "def simple_nn_model(input_shape):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "  x = layers.Dense(30, activation='relu')(inputs)\n",
        "  x = layers.Dropout(0.4)(x)\n",
        "  x = layers.Dense(20, activation='relu')(x)\n",
        "  x = layers.Dropout(0.4)(x)\n",
        "  x = layers.Dense(10, activation='relu')(x)\n",
        "  x = layers.Dropout(0.4)(x)\n",
        "  # x = layers.Dense(run['layer_sizes'][2], activation='relu')(x)\n",
        "  # x = layers.Dropout(run['dropout_factor'])(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "\n",
        "# def conv_nn_model(input_shape=225):\n",
        "#   inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "#   intermediate = tf.keras.layers.Reshape((225,1), input_shape=(225,))(inputs)\n",
        "\n",
        "#   first_third = tf.keras.layers.Cropping1D(cropping=(0,150))(intermediate)\n",
        "#   first_third = tf.keras.layers.Reshape((75,), input_shape=(75,1))(first_third)\n",
        "\n",
        "#   # second_half = tf.keras.layers.Cropping1D(cropping=(300,0))(intermediate)\n",
        "#   # second_half = tf.keras.layers.Reshape((200,), input_shape=(200,1))(second_half)\n",
        "#   x = layers.Reshape((5, 5, 3))(first_third)\n",
        "#   resnet = tf.keras.applications.resnet50.ResNet50(\n",
        "#       include_top=False, weights='imagenet', input_tensor=inputs,\n",
        "#       input_shape=(5, 5, 3), pooling='avg')\n",
        "#   for i in resnet.layers:\n",
        "#       i.trainable = False\n",
        "#   x = resnet(x)\n",
        "#   x = layers.Dense(20, activation='relu')(inputs)\n",
        "#   x = layers.Dropout(0)(x)\n",
        "\n",
        "#   outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "#   model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "#   print(model.summary())\n",
        "\n",
        "#   return model\n",
        "\n",
        "def conv_nn_model(input_shape):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "  x = layers.Reshape((5, 5, n))(inputs)\n",
        "\n",
        "  x = layers.Conv2D(32, (2, 2), strides = (1,1), input_shape=(5, 5, n))(x)\n",
        "  x = layers.MaxPooling2D((2, 2))(x)\n",
        "  # x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "  # x = layers.MaxPooling2D((2, 2))(x)\n",
        "  # x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(16, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(8, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "def avg_nn_model(input_shape):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "  x = layers.Reshape((5, 5, n))(inputs)\n",
        "  avg_x = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2]))(x)\n",
        "  max_x = tf.keras.layers.Lambda(lambda x: tf.reduce_max(x, axis=[1, 2]))(x)\n",
        "  min_x = tf.keras.layers.Lambda(lambda x: tf.reduce_min(x, axis=[1, 2]))(x)\n",
        "  # diff_x = tf.keras.layers.Subtract()([max_x, min_x])\n",
        "  std_x = tf.keras.layers.Lambda(lambda x: tf.math.reduce_std(x, axis=[1, 2]))(x)\n",
        "\n",
        "  x = tf.keras.layers.Concatenate(axis=1)([avg_x, std_x, max_x, min_x])\n",
        "\n",
        "  # x = tf.reduce_mean(x, axis=[1, 2])\n",
        "\n",
        "  # x = layers.Conv2D(32, (2, 2), strides = (1,1), input_shape=(5, 5, n))(x)\n",
        "  # x = layers.MaxPooling2D((2, 2))(x)\n",
        "  # x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "  # x = layers.MaxPooling2D((2, 2))(x)\n",
        "  # x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "\n",
        "\n",
        "  # x = layers.Flatten()(x)\n",
        "  x = layers.Dense(16, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(8, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# my_model.fit(X_train, y_train)\n",
        "\n",
        "model = avg_nn_model(shape)"
      ],
      "metadata": {
        "id": "XW4npS4SAadC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P1Vpw-3Wb83Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "for model_i in [conv_nn_model]:\n",
        "  # f1_scores = []\n",
        "  # for train_index, test_index in kf.split(X):\n",
        "  #   # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  #   X_train, X_test = X[train_index], X[test_index]\n",
        "  #   y_train, y_test = y[train_index], y[test_index]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=2022)\n",
        "  \n",
        "  model = model_i(shape)\n",
        "\n",
        "  loss = tf.keras.losses.binary_crossentropy\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  metrics = [tf.keras.metrics.BinaryAccuracy(name='test_accuracy'),\n",
        "              tf.keras.metrics.Recall(name='recall'),\n",
        "              tf.keras.metrics.Precision(name='precision'),\n",
        "              tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "              tf.keras.metrics.FalsePositives(name='fp'),\n",
        "              tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "              tf.keras.metrics.TruePositives(name='tp')]\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=metrics)\n",
        "  \n",
        "  # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-3,\n",
        "  #                                                           restore_best_weights=True)\n",
        "\n",
        "  history_simple = model.fit(X_train, y_train, validation_data = [X_test, y_test], epochs=20, \n",
        "                            verbose=2, class_weight=class_weight)\n",
        "  \n",
        "  f1_score = calculate_f1(np.array(history_simple.history['val_precision']), np.array(history_simple.history['val_recall']))[-1]\n",
        "\n",
        "  # f1_scores.append(f1_score)\n",
        "  print(f1_score)\n",
        "  # print(f'mean: {np.mean(f1_scores)}')"
      ],
      "metadata": {
        "id": "RV6uZ3sSAwxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor, XGBClassifier"
      ],
      "metadata": {
        "id": "UkdnxXfpj-1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=2022)\n",
        "xgboost_model = XGBRegressor(n_estimators=500, learning_rate=0.05)\n",
        "xgboost_model.fit(X_train, y_train, \n",
        "             early_stopping_rounds=5, \n",
        "             eval_set=[(X_test, y_test)], \n",
        "             verbose=False)\n",
        "# xgboost_model = lambda x: np.sigmoid(xgboost_model(x))"
      ],
      "metadata": {
        "id": "Sw3fV3y3DE2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xgboost_model = XGBClassificator(n_estimators=500, learning_rate=0.05)\n",
        "# xgboost_model.fit(X_train, y_train, \n",
        "#              )\n",
        "xgb_cl = XGBClassifier(class_weight='balanced')\n",
        "\n",
        "# Fit\n",
        "xgb_cl.fit(X_train, y_train, early_stopping_rounds=5, \n",
        "             eval_set=[(X_test, y_test)], \n",
        "             verbose=False)"
      ],
      "metadata": {
        "id": "HKIKpApbjSPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_model = RandomForestClassifier(n_estimators = 100, random_state = 2022, class_weight='balanced')\n",
        "rfc_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "s1jpPTBiWID3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jlYlRSkveN2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors=20)\n",
        "knn_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "4G2MhR5ZfB7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_model = SVC(probability=True, kernel='rbf', class_weight='balanced')\n",
        "svc_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "n_hKVq72fCKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=2022)\n",
        "avg_model = avg_nn_model(shape)\n",
        "\n",
        "loss = tf.keras.losses.binary_crossentropy\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "metrics = [tf.keras.metrics.BinaryAccuracy(name='test_accuracy'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "            tf.keras.metrics.FalsePositives(name='fp'),\n",
        "            tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "            tf.keras.metrics.TruePositives(name='tp')]\n",
        "\n",
        "avg_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-3,\n",
        "#                                                           restore_best_weights=True)\n",
        "\n",
        "history_avg = avg_model.fit(X_train, y_train, validation_data = [X_test, y_test], epochs=20, \n",
        "                          verbose=2, class_weight=class_weight)\n",
        "\n",
        "f1_score = calculate_f1(np.array(history_avg.history['val_precision']), np.array(history_simple.history['val_recall']))[-1]\n",
        "\n",
        "# f1_scores.append(f1_score)\n",
        "print(f1_score)"
      ],
      "metadata": {
        "id": "RnhfIGchQBod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr -> 0.001"
      ],
      "metadata": {
        "id": "6pUVT9hN72a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist_f1(history):\n",
        "  hist_f1 = calculate_f1(np.array(history.history['precision']), np.array(history.history['recall']))\n",
        "  hist_f1_val = calculate_f1(np.array(history.history['val_precision']), np.array(history.history['val_recall']))\n",
        "  plt.plot(hist_f1, label = 'train')\n",
        "  plt.plot(hist_f1_val, label ='test')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('f1_score')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "def calculate_f1(precision, recall):\n",
        "  return 2 * (precision * recall)/ (precision + recall)\n",
        "plot_hist_f1(history_simple)\n",
        "\n",
        "\n",
        "\n",
        "# print(history.history['precision'] .* history.history['recall'])"
      ],
      "metadata": {
        "id": "AwxBJ6iKCtDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_predictions(model_predictions, weights):\n",
        "  predictions = np.zeros((len(model_predictions['AVG'].flatten(),)))\n",
        "  print(len(predictions))\n",
        "\n",
        "  for weight, prediction in zip(weights, model_predictions.values()):\n",
        "    predictions = predictions + weight * np.array(prediction.flatten())\n",
        "  return predictions\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "wbjtz4-mR7Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgboost_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "azzAeeLunOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = lambda x: 1/(1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "HIv1MK5jxMmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "model_predictions = {'Regr': xgboost_model.predict(X_test), \n",
        "                     'Conv': model.predict(X_test), \n",
        "                     'AVG': avg_model.predict(X_test), \n",
        "                     'cli': xgb_cl.predict_proba(X_test)[:, 1],\n",
        "                     'rfc': rfc_model.predict_proba(X_test)[:, 1],\n",
        "                     'gnb': gnb_model.predict_proba(X_test)[:, 1],\n",
        "                     'knn': knn_model.predict_proba(X_test)[:, 1],\n",
        "                     'svc': svc_model.predict_proba(X_test)[:, 1]}\n",
        "weights = [0.3, 0.05, 0.05, 0.05, 0.3, 0.05, 0.05, 0.15]\n",
        "thres = 0.38\n",
        "# y_pred_xg = xgboost_model.predict(X_test)\n",
        "# y_pred = model.predict(X_test)\n",
        "# y_pred_avg = avg_model.predict(X_test)\n",
        "# y_pred_res = np.mean([np.array(y_pred_xg.flatten()), np.array(y_pred.flatten()), np.array(y_pred_avg.flatten())], axis=0)\n",
        "y_pred_res = combined_predictions(model_predictions, weights)\n",
        "\n",
        "cm = confusion_matrix(y_test, [1 if x >= thres else 0 for x in y_pred_res.flatten() ])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "disp.plot(ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nEBHvuifsNW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_res)"
      ],
      "metadata": {
        "id": "r-tev5UxgybG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_xg"
      ],
      "metadata": {
        "id": "dEv2V0aAb_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance\n",
        "# impo_df = pd.DataFrame({'feature': X.columns, 'importance': model.feature_importances_}).set_index('feature').sort_values(by = 'importance', ascending = False)\n",
        "# impo_df = impo_df[:10].sort_values(by = 'importance', ascending = True)\n",
        "# impo_df.plot(kind = 'barh', figsize = (10, 10))\n",
        "# plt.legend(loc = 'center right')\n",
        "# plt.title('Bar chart showing top ten features', fontsize = 14)\n",
        "# plt.xlabel('Features', fontsize = 12, color = 'indigo')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_VLRzyCytDBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Predictions\"></a>\n",
        "## 10. Test set predictions"
      ],
      "metadata": {
        "id": "4R_WA5C5D-2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction on the test set\n",
        "# main_cols = test_new.columns[1:]\n",
        "\n",
        "test_df = test_new[X_test.columns]\n",
        "model_predictions = {'Regr': xgboost_model.predict(test_df), \n",
        "                     'Conv': model.predict(test_df), \n",
        "                     'AVG': avg_model.predict(test_df), \n",
        "                     'cli': xgb_cl.predict_proba(test_df)[:, 1],\n",
        "                     'rfc': rfc_model.predict_proba(test_df)[:, 1],\n",
        "                     'gnb': gnb_model.predict_proba(test_df)[:, 1],\n",
        "                     'knn': knn_model.predict_proba(test_df)[:, 1],\n",
        "                     'svc': svc_model.predict_proba(test_df)[:, 1]}\n",
        "# y_pred_xg = xgboost_model.predict(test_df)\n",
        "# y_pred = model.predict(test_df)\n",
        "# y_pred_avg = avg_model.predict(test_df)\n",
        "# y_pred_res = np.mean([np.array(y_pred_xg.flatten()), np.array(y_pred.flatten()), np.array(y_pred_avg.flatten())], axis=0)\n",
        "y_pred_res = combined_predictions(model_predictions, weights)\n",
        "# Create a submission file\n",
        "sub_file = pd.DataFrame({'Sample_ID': test_new.Sample_ID, 'Label': [1 if x >= thres else 0 for x in y_pred_res.flatten() ]})\n",
        "\n",
        "# Check the distribution of your predictions\n",
        "sns.countplot(x = sub_file.Label)\n",
        "plt.title('Predicted Variable Distribution');"
      ],
      "metadata": {
        "id": "wIUx265DD_ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Submission\"></a>\n",
        "## 11. Creating a submission file"
      ],
      "metadata": {
        "id": "gQK6hJQHJD5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a csv file and upload to zindi \n",
        "sub_file.to_csv('submission.csv', index = False)\n",
        "sub_file"
      ],
      "metadata": {
        "id": "VGaH2ZZLrknb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = \"Tips\"></a>\n",
        "## 12. Tips to improve model performance\n",
        " - Use cross-validation techniques\n",
        " - Feature engineering\n",
        " - Handle the class imbalance of the target variable\n",
        " - Try different modelling techniques - Stacking classifier, Voting classifiers, ensembling...\n",
        " - Data transformations\n",
        " - Feature Selection techniques such as RFE, Tree-based feature importance...\n",
        " - Domain Knowledge, do research on how the provided features affect landslides, soil topology..."
      ],
      "metadata": {
        "id": "2Md0cnDIJwiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#                       ::GOOD LUCK AND HAPPY HACKING \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3io5PEySK5x0"
      }
    }
  ]
}